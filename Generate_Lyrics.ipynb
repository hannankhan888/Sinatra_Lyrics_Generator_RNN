{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "261c5497",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T00:22:52.284519Z",
     "start_time": "2022-05-18T00:22:50.132228Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "269e7387",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T00:22:52.299478Z",
     "start_time": "2022-05-18T00:22:52.285516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(text) = 705123\n",
      "Example: fly me to the moon let me play among the stars and\n"
     ]
    }
   ],
   "source": [
    "data_path = r'data'\n",
    "filename = 'Lyrics_FrankSinatra.txt'\n",
    "filepath = os.sep.join([data_path, filename])\n",
    "checkpoint_path = r'checkpoints'\n",
    "\n",
    "text = ''\n",
    "with open(filepath, encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"len(text) = {len(text)}\")\n",
    "print(\"Example:\", text[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42e10cd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T00:22:52.315436Z",
     "start_time": "2022-05-18T00:22:52.300476Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(vocab) = 27\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print(f\"len(vocab) = {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4236b1b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T00:22:52.362310Z",
     "start_time": "2022-05-18T00:22:52.316433Z"
    }
   },
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)\n",
    "chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(),\n",
    "                                             invert=True, mask_token=None)\n",
    "\n",
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e3bd5bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T00:22:52.378268Z",
     "start_time": "2022-05-18T00:22:52.363308Z"
    }
   },
   "outputs": [],
   "source": [
    "sequence_len = 100\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62cf5257",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T00:22:52.393228Z",
     "start_time": "2022-05-18T00:22:52.379265Z"
    }
   },
   "outputs": [],
   "source": [
    "class LyricsGeneratorModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=sequence_len)\n",
    "        self.gru = tf.keras.layers.GRU(rnn_units, return_sequences=True, return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "        \n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f30d6a1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T00:22:53.307071Z",
     "start_time": "2022-05-18T00:22:52.394225Z"
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS = 80\n",
    "\n",
    "# Load model\n",
    "model = LyricsGeneratorModel(vocab_size=vocab_size,\n",
    "                            embedding_dim=embedding_dim,\n",
    "                            rnn_units=rnn_units)\n",
    "model.build((None, 100))\n",
    "model.load_weights(f\"model_lower_char_{EPOCHS}_epochs_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05f6b93f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T00:22:53.322032Z",
     "start_time": "2022-05-18T00:22:53.308069Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a mask to prevent \"[UNK]\" from being generated.\n",
    "skip_ids = ids_from_chars(['[UNK]'])[:, None]\n",
    "sparse_mask = tf.SparseTensor(\n",
    "    # Put a -inf at each bad index.\n",
    "    values=[-float('inf')]*len(skip_ids),\n",
    "    indices=skip_ids,\n",
    "    # Match the shape to the vocabulary\n",
    "    dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "def generate_one_step(inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = model(inputs=input_ids, states=states,\n",
    "                                          return_state=True, training=False)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/1.0\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b1d9498",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T00:22:59.776418Z",
     "start_time": "2022-05-18T00:22:53.323029Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "life begin so open up your heart and let this fool rush inhey drink up all you people order anything you see and have fun you happy people when the new heaven and earth shall own thy eyes those ill remember april and ill smiles posing i should fall in love with you evry day the thrill is always new evry day the morning sparkles spang at the sky now ive met miss jones and well keep on meeting till we die miss jones and id always ask for you amor meet up and down my spine the same old witchcrffa kiss the mist of day never never to know how i love you if i loved you brief instrumental soon youd leave me oh my dindi oh home not quictly cry dont night thats you and me is us with your heart it isnt smart it is no disfrceling for the gal who got that magic for you my heart got those jooms at tweet the birds that flew it say is this the day i alove all or its open and your favorite scone i cant have an echomain things to be seen when the wind is greenin in julets down or sunny weather the outlook  \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 6.440423965454102\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['life'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12597ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
